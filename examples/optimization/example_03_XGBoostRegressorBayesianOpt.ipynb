{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 03: General Use of XGBoostRegressorBayesianOpt\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slickml/slick-ml/blob/master/examples/optimization/example_03_XGBoostRegressorBayesianOpt.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/slickml/slick-ml.git\n",
    "# %cd slick-ml\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amirhessam/Documents/GitHub/slick-ml\n"
     ]
    }
   ],
   "source": [
    "# # Change path to project root\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# widen the screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# change the path and loading class\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from slickml.optimization import XGBoostRegressorBayesianOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# XGBoostRegressorBayesianOpt Docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBoostRegressorBayesianOpt in module slickml.optimization:\n",
      "\n",
      "class XGBoostRegressorBayesianOpt(slickml.regression.XGBoostCVRegressor)\n",
      " |  XGBoostRegressorBayesianOpt(n_iter=None, init_points=None, acq=None, pbounds=None, num_boost_round=None, n_splits=None, metrics=None, objective=None, early_stopping_rounds=None, random_state=None, shuffle=True, sparse_matrix=False, scale_mean=False, scale_std=False, importance_type=None, verbose=True)\n",
      " |  \n",
      " |  XGBoost Hyper-Parameters Tunning using Bayesian Optimization.\n",
      " |  This is wrapper using Bayesian Optimization to tune the parameters\n",
      " |  for XGBoost Regressor using xgboost.cv() model with n-folds\n",
      " |  cross-validation iteratively. This function is pretty useful find\n",
      " |  the optimized set of parameters before training. Please note that,\n",
      " |  the optimizier objective is always to maximize the target. Therefore,\n",
      " |  in case of using a metric such as logloss or error, the negative value\n",
      " |  of the metric will be maximized.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_iter: int, optional (default=5)\n",
      " |      Number of iteration rounds for hyper-parameters tuning\n",
      " |      \n",
      " |  init_points: int, optional (default=5)\n",
      " |      Number of initial points to initialize the optimizer\n",
      " |      \n",
      " |  acq: str, optional (default=\"ei\")\n",
      " |      Type of acquisition function such as expected improvement (ei)\n",
      " |      \n",
      " |  pbounds: dict, optional\n",
      " |      Set of parameters boundaries for Bayesian Optimization\n",
      " |      (default={\"max_depth\" : (2, 7),\n",
      " |                \"learning_rate\" : (0, 1),\n",
      " |                \"min_child_weight\" : (1, 20),\n",
      " |                \"colsample_bytree\": (0.1, 1.0)\n",
      " |                \"subsample\" : (0.1, 1),\n",
      " |                \"gamma\" : (0, 1),\n",
      " |                \"reg_alpha\" : (0, 1),\n",
      " |                \"reg_lambda\" : (0, 1)})\n",
      " |                \n",
      " |  num_boost_round: int, optional (default=200)\n",
      " |      Number of boosting round at each fold of xgboost.cv()\n",
      " |      \n",
      " |  n_splits: int, optional (default=4)\n",
      " |      Number of folds for cross-validation\n",
      " |      \n",
      " |  metrics: str or tuple[str], optional (default=(\"rmse\"))\n",
      " |      Metric used for evaluation at cross-validation\n",
      " |      using xgboost.cv(). Please note that this is different\n",
      " |      than eval_metric that needs to be passed to params dict.\n",
      " |      Possible values are \"rmse\", \"rmsle\", \"mae\"\n",
      " |      \n",
      " |  objective: str, optional (default=\"reg:squarederror\")\n",
      " |      Type of objective function regression;\n",
      " |      Other options for objective: \"reg:logistic\", \"reg:squaredlogerror\"\n",
      " |      \n",
      " |  early_stopping_rounds: int, optional (default=20)\n",
      " |      The criterion to early abort the xgboost.cv() phase\n",
      " |      if the test metric is not improved\n",
      " |      \n",
      " |  random_state: int, optional (default=1367)\n",
      " |      Random seed\n",
      " |      \n",
      " |  shuffle: bool, optional (default=True)\n",
      " |      Flag to shuffle data to have the ability of building\n",
      " |      stratified folds in xgboost.cv()\n",
      " |      \n",
      " |  sparse_matrix: bool, optional (default=False)\n",
      " |      Flag to convert data to sparse matrix with csr format.\n",
      " |      This would increase the speed of feature selection for\n",
      " |      relatively large datasets\n",
      " |      \n",
      " |  scale_mean: bool, optional (default=False)\n",
      " |      Flag to center the data before scaling. This flag should be\n",
      " |      False when using sparse_matrix=True, since it centering the data\n",
      " |      would decrease the sparsity and in practice it does not make any\n",
      " |      sense to use sparse matrix method and it would make it worse.\n",
      " |      \n",
      " |  scale_std: bool, optional (default=False)\n",
      " |      Flag to scale the data to unit variance\n",
      " |      (or equivalently, unit standard deviation)\n",
      " |      \n",
      " |  importance_type: str, optional (default=\"total_gain\")\n",
      " |      Importance type of xgboost.train() with possible values\n",
      " |      \"weight\", \"gain\", \"total_gain\", \"cover\", \"total_cover\"\n",
      " |      \n",
      " |  verbose: bool, optional (default=True)\n",
      " |      Flag to show the Bayesian Optimization progress at each iteration\n",
      " |      \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scaler_: StandardScaler object\n",
      " |      Returns the scaler object if any of scale_mean or scale_std\n",
      " |      was passed True.\n",
      " |      \n",
      " |  X_train_: pandas.DataFrame\n",
      " |      Returns scaled training data set that passed if if any of\n",
      " |      scale_mean or scale_std was passed as True, else X_train.\n",
      " |      \n",
      " |  d_train_: xgboost.DMatrix object\n",
      " |      Returns the xgboost.DMatrix(X_train_, y_train)\n",
      " |      \n",
      " |  optimizer_: Bayesian Optimiziation object\n",
      " |      Returns the fitted optimizer on (X_train_, y_train)\n",
      " |      \n",
      " |  optimization_results_: Optimization results Pandas DataFrame()\n",
      " |      Returns all the optimization results including target and params\n",
      " |      \n",
      " |  best_params_: Tuned xgboost params dict\n",
      " |      Returns the tuned params dict\n",
      " |      \n",
      " |  best_performance_: Target value and tuned params Pandas DataFrame()\n",
      " |      Return the dataframe of the best performance\n",
      " |      \n",
      " |  fit(X_train, y_train): instance method\n",
      " |      Returns None and applies the optimization process using\n",
      " |      the (X_train, y_train) set using xgboost.cv() and Bayesian Opt\n",
      " |      \n",
      " |  plot_optimization_results(): instance method\n",
      " |      Plot all the optimization results\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBoostRegressorBayesianOpt\n",
      " |      slickml.regression.XGBoostCVRegressor\n",
      " |      slickml.regression.XGBoostRegressor\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_iter=None, init_points=None, acq=None, pbounds=None, num_boost_round=None, n_splits=None, metrics=None, objective=None, early_stopping_rounds=None, random_state=None, shuffle=True, sparse_matrix=False, scale_mean=False, scale_std=False, importance_type=None, verbose=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X_train, y_train)\n",
      " |      Function to run xgboost.cv() method first to find the best number of boosting round\n",
      " |      and train a model based on that on (X_train, y_train) set and returns it.\n",
      " |  \n",
      " |  get_best_params(self)\n",
      " |      Function to return the best (tuned) set of hyper-parameters.\n",
      " |  \n",
      " |  get_best_performance(self)\n",
      " |      Function to return the performance of the best (tuned)\n",
      " |      set of hyper-parameters.\n",
      " |  \n",
      " |  get_optimization_results(self)\n",
      " |      Function to return the optimization results.\n",
      " |  \n",
      " |  get_optimizer(self)\n",
      " |      Function to return the Bayesian Optimization object.\n",
      " |  \n",
      " |  get_pbounds(self)\n",
      " |      Function to return the hyper-parameters bounds.\n",
      " |  \n",
      " |  plot_optimization_results(self)\n",
      " |      Function to plot the optimization results.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from slickml.regression.XGBoostCVRegressor:\n",
      " |  \n",
      " |  get_cv_results(self)\n",
      " |      Function to return both internal and external cross-validation\n",
      " |      results as Pandas DataFrame().\n",
      " |  \n",
      " |  plot_cv_results(self, figsize=None, linestyle=None, train_label=None, test_label=None, train_color=None, train_std_color=None, test_color=None, test_std_color=None, save_path=None)\n",
      " |      Function to plot the results of xgboost.cv() process and evolution\n",
      " |      of metrics through number of boosting rounds.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cv_results: Pandas DataFrame()\n",
      " |          Cross-validation results in DataFrame() format\n",
      " |      \n",
      " |      figsize: tuple, optional, (default=(8, 5))\n",
      " |          Figure size\n",
      " |      \n",
      " |      linestyle: str, optional, (default=\"--\")\n",
      " |          Style of lines. Complete options are available at\n",
      " |          (https://matplotlib.org/3.1.0/gallery/lines_bars_and_markers/linestyles.html)\n",
      " |      \n",
      " |      train_label: str, optional (default=\"Train\")\n",
      " |          Label in the figure legend for the training line\n",
      " |      \n",
      " |      test_label: str, optional (default=\"Test\")\n",
      " |          Label in the figure legend for the training line\n",
      " |      \n",
      " |      train_color: str, optional, (default=\"navy\")\n",
      " |          Color of the training line\n",
      " |      \n",
      " |      train_std_color: str, optional, (default=\"#B3C3F3\")\n",
      " |          Color of the edge color of the training std bars\n",
      " |      \n",
      " |      test_color: str, optional, (default=\"purple\")\n",
      " |          Color of the testing line\n",
      " |      \n",
      " |      test_std_color: str, optional, (default=\"#D0AAF3\")\n",
      " |          Color of the edge color of the testing std bars\n",
      " |      \n",
      " |      save_path: str, optional (default=None)\n",
      " |          The full or relative path to save the plot including the image format.\n",
      " |          For example \"myplot.png\" or \"../../myplot.pdf\"\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from slickml.regression.XGBoostRegressor:\n",
      " |  \n",
      " |  get_feature_importance(self)\n",
      " |      Function to return the feature importance of the best model\n",
      " |      at each fold of each iteration of feature selection.\n",
      " |  \n",
      " |  get_params(self)\n",
      " |      Function to return the train parameters for XGBoost.\n",
      " |  \n",
      " |  plot_feature_importance(self, figsize=None, color=None, marker=None, markersize=None, markeredgecolor=None, markerfacecolor=None, markeredgewidth=None, fontsize=None, save_path=None)\n",
      " |      Function to plot XGBoost feature importance.\n",
      " |      This function is a helper function based on the feature_importance_\n",
      " |      attribute of the XGBoostRegressor class.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature importance: Pandas DataFrame\n",
      " |          Feature frequency\n",
      " |      \n",
      " |      figsize: tuple, optional, (default=(8, 5))\n",
      " |          Figure size\n",
      " |      \n",
      " |      color: str, optional, (default=\"#87CEEB\")\n",
      " |          Color of the vertical lines of lollipops\n",
      " |      \n",
      " |      marker: str, optional, (default=\"o\")\n",
      " |          Market style of the lollipops. Complete valid\n",
      " |          marker styke can be found at:\n",
      " |          (https://matplotlib.org/2.1.1/api/markers_api.html#module-matplotlib.markers)\n",
      " |      \n",
      " |      markersize: int or float, optional, (default=10)\n",
      " |          Markersize\n",
      " |      \n",
      " |      markeredgecolor: str, optional, (default=\"1F77B4\")\n",
      " |          Marker edge color\n",
      " |      \n",
      " |      markerfacecolor: str, optional, (default=\"1F77B4\")\n",
      " |          Marker face color\n",
      " |      \n",
      " |      markeredgewidth: int or float, optional, (default=1)\n",
      " |          Marker edge width\n",
      " |      \n",
      " |      fontsize: int or float, optional, (default=12)\n",
      " |          Fontsize for xlabel and ylabel, and ticks parameters\n",
      " |      \n",
      " |      save_path: str, optional (default=None)\n",
      " |          The full or relative path to save the plot including the image format.\n",
      " |          For example \"myplot.png\" or \"../../myplot.pdf\"\n",
      " |  \n",
      " |  plot_shap_summary(self, validation=True, plot_type=None, figsize=None, color=None, max_display=None, feature_names=None, title=None, show=True, sort=True, color_bar=True, layered_violin_max_num_bins=None, class_names=None, class_inds=None, color_bar_label=None, save_path=None)\n",
      " |      Function to plot shap summary plot.\n",
      " |      This function is a helper function to plot the shap summary plot\n",
      " |      based on all types of shap explainers including tree, linear, and dnn.\n",
      " |      Please note that this function should be ran after the predict_proba to\n",
      " |      make sure the X_test is being instansiated.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      validation: bool, optional, (default=True)\n",
      " |          Flag to calculate SHAP values of X_test if it is True.\n",
      " |          If validation=False, it calculates the SHAP values of\n",
      " |          X_train and plots the summary plot.\n",
      " |      \n",
      " |      plot_type: str, optional (single-output default=\"dot\", multi-output default=\"bar\")\n",
      " |          The type of summar plot. Options are \"bar\", \"dot\", \"violin\", \"layered_violin\",\n",
      " |          and \"compact_dot\" which is recommended for SHAP interactions\n",
      " |      \n",
      " |      figsize: tuple, optional, (default=\"auto\")\n",
      " |          Figure size\n",
      " |      \n",
      " |      color: str, optional, (default= \"#D0AAF3\" for \"bar\")\n",
      " |          Color of violin and layered violin plots are \"RdBl\" cmap\n",
      " |          Color of the horizontal lines when plot_type=\"bar\" is \"#D0AAF3\"\n",
      " |      \n",
      " |      max_display: int, optional, (default=20)\n",
      " |          Limit to show the number of features in the plot\n",
      " |      \n",
      " |      feature_names: str, optional, (default=None)\n",
      " |          List of feature names to pass. It should follow the order\n",
      " |          of fatures\n",
      " |      \n",
      " |      title: str, optional, (default=None)\n",
      " |          Title of the plot\n",
      " |      \n",
      " |      show: bool, optional, (default=True)\n",
      " |          Flag to show the plot in inteactive environment\n",
      " |      \n",
      " |      sort: bool, optional, (default=True)\n",
      " |          Flag to plot sorted shap vlues in descending order\n",
      " |      \n",
      " |      color_bar: bool, optional, (default=True)\n",
      " |          Flag to show color_bar when plot_type is \"dot\" or \"violin\"\n",
      " |      \n",
      " |      layered_violin_max_num_bins: int, optional, (default=10)\n",
      " |          The number of bins for calculating the violin plots ranges\n",
      " |          and outliers\n",
      " |      \n",
      " |      class_names: list, optional, (default=None)\n",
      " |          List of class names for multi-output problems\n",
      " |      \n",
      " |      class_inds: list, optional, (default=True)\n",
      " |          List of class indices for multi-output problems\n",
      " |      \n",
      " |      color_bar_label: str, optional, (default=\"Feature Value\")\n",
      " |          Label for color bar\n",
      " |      \n",
      " |      save_path: str, optional (default=None)\n",
      " |          The full or relative path to save the plot including the image format.\n",
      " |          For example \"myplot.png\" or \"../../myplot.pdf\"\n",
      " |  \n",
      " |  plot_shap_waterfall(self, validation=True, figsize=None, bar_color=None, bar_thickness=None, line_color=None, marker=None, markersize=None, markeredgecolor=None, markerfacecolor=None, markeredgewidth=None, max_display=None, title=None, fontsize=None, save_path=None)\n",
      " |      Function to plot shap waterfall plot.\n",
      " |      This function is a helper function to plot the shap waterfall plot\n",
      " |      based on all types of shap explainers including tree, linear, and dnn.\n",
      " |      This would show the cumulitative/composite ratios of shap values per feature.\n",
      " |      Therefore, it can be easily seen with each feature how much explainability we\n",
      " |      can acheieve. Please note that this function should be ran after the predict_proba to\n",
      " |      make sure the X_test is being instansiated.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      validation: bool, optional, (default=True)\n",
      " |          Flag to calculate SHAP values of X_test if it is True.\n",
      " |          If validation=False, it calculates the SHAP values of\n",
      " |          X_train and plots the summary plot.\n",
      " |      \n",
      " |      figsize: tuple, optional, (default=(8, 5))\n",
      " |          Figure size\n",
      " |      \n",
      " |      bar_color: str, optional, (default=\"#B3C3F3\")\n",
      " |          Color of the horizontal bar lines\n",
      " |      \n",
      " |      bar_thickness: float, optional, (default=0.5)\n",
      " |          Thickness (hight) of the horizontal bar lines\n",
      " |      \n",
      " |      line_color: str, optional, (default=\"purple\")\n",
      " |          Color of the line plot\n",
      " |      \n",
      " |      marker: str, optional, (default=\"o\")\n",
      " |          Marker style\n",
      " |          marker style can be found at:\n",
      " |          (https://matplotlib.org/2.1.1/api/markers_api.html#module-matplotlib.markers)\n",
      " |      \n",
      " |      markersize: int or float, optional, (default=7)\n",
      " |          Markersize\n",
      " |      \n",
      " |      markeredgecolor: str, optional, (default=\"purple\")\n",
      " |          Marker edge color\n",
      " |      \n",
      " |      markerfacecolor: str, optional, (default=\"purple\")\n",
      " |          Marker face color\n",
      " |      \n",
      " |      markeredgewidth: int or float, optional, (default=1)\n",
      " |          Marker edge width\n",
      " |      \n",
      " |      max_display: int, optional, (default=20)\n",
      " |          Limit to show the number of features in the plot\n",
      " |      \n",
      " |      title: str, optional, (default=None)\n",
      " |          Title of the plot\n",
      " |      \n",
      " |      fontsize: int or float, optional, (default=12)\n",
      " |          Fontsize for xlabel and ylabel, and ticks parameters\n",
      " |      \n",
      " |      save_path: str, optional (default=None)\n",
      " |          The full or relative path to save the plot including the image format.\n",
      " |          For example \"myplot.png\" or \"../../myplot.pdf\"\n",
      " |  \n",
      " |  predict(self, X_test, y_test=None)\n",
      " |      Function to return the prediction of target values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X_test: numpy.array or pandas.DataFrame\n",
      " |          Validation features data\n",
      " |      \n",
      " |      y_test: numpy.array[int] or list[int], optional (default=None)\n",
      " |          List of validation ground truth binary values [0, 1]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from slickml.regression.XGBoostRegressor:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(XGBoostRegressorBayesianOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>TARGET1</th>\n",
       "      <th>TARGET2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.138</td>\n",
       "      <td>1.543333</td>\n",
       "      <td>289964.0</td>\n",
       "      <td>22.491483</td>\n",
       "      <td>111.289667</td>\n",
       "      <td>7584.0</td>\n",
       "      <td>7584.0</td>\n",
       "      <td>737.156</td>\n",
       "      <td>561.15</td>\n",
       "      <td>823.713</td>\n",
       "      <td>109600.0</td>\n",
       "      <td>99800.0</td>\n",
       "      <td>594700.0</td>\n",
       "      <td>101900.0</td>\n",
       "      <td>0.07137</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.088</td>\n",
       "      <td>3.086667</td>\n",
       "      <td>6960180.0</td>\n",
       "      <td>22.936100</td>\n",
       "      <td>113.807817</td>\n",
       "      <td>28204.0</td>\n",
       "      <td>28204.0</td>\n",
       "      <td>908.551</td>\n",
       "      <td>561.15</td>\n",
       "      <td>854.808</td>\n",
       "      <td>133100.0</td>\n",
       "      <td>99800.0</td>\n",
       "      <td>728200.0</td>\n",
       "      <td>101900.0</td>\n",
       "      <td>0.10655</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      F1        F2         F3         F4          F5       F6       F7  \\\n",
       "0  1.138  1.543333   289964.0  22.491483  111.289667   7584.0   7584.0   \n",
       "1  2.088  3.086667  6960180.0  22.936100  113.807817  28204.0  28204.0   \n",
       "\n",
       "        F8      F9      F10       F11      F12       F13       F14      F15  \\\n",
       "0  737.156  561.15  823.713  109600.0  99800.0  594700.0  101900.0  0.07137   \n",
       "1  908.551  561.15  854.808  133100.0  99800.0  728200.0  101900.0  0.10655   \n",
       "\n",
       "     F16  TARGET1  TARGET2  \n",
       "0  0.082     0.95    0.975  \n",
       "1  0.287     0.95    0.975  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data; note this is a multi regression data\n",
    "df = pd.read_csv(\"data/reg_data.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define X, y based on one of the targets\n",
    "y = df.TARGET1.values\n",
    "X = df.drop([\"TARGET1\", \"TARGET2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=1367)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bayesian optimizer \n",
    "xbo = XGBoostRegressorBayesianOpt(n_iter=10,\n",
    "                                  init_points=5,\n",
    "                                  metrics=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.8975  \u001b[0m | \u001b[0m 0.04571 \u001b[0m | \u001b[0m 0.6628  \u001b[0m | \u001b[0m 4.238   \u001b[0m | \u001b[0m 1.436   \u001b[0m | \u001b[0m 0.3064  \u001b[0m | \u001b[0m 0.7136  \u001b[0m | \u001b[0m 0.1931  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.7904  \u001b[0m | \u001b[0m 0.6447  \u001b[0m | \u001b[0m 0.9152  \u001b[0m | \u001b[0m 3.334   \u001b[0m | \u001b[0m 3.238   \u001b[0m | \u001b[0m 0.7772  \u001b[0m | \u001b[0m 0.269   \u001b[0m | \u001b[0m 0.9726  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.8498  \u001b[0m | \u001b[0m 0.6044  \u001b[0m | \u001b[0m 0.6874  \u001b[0m | \u001b[0m 6.651   \u001b[0m | \u001b[0m 15.7    \u001b[0m | \u001b[0m 0.061   \u001b[0m | \u001b[0m 0.5114  \u001b[0m | \u001b[0m 0.6848  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.7297  \u001b[0m | \u001b[0m 0.8513  \u001b[0m | \u001b[0m 0.4627  \u001b[0m | \u001b[0m 4.757   \u001b[0m | \u001b[0m 4.965   \u001b[0m | \u001b[0m 0.9328  \u001b[0m | \u001b[0m 0.363   \u001b[0m | \u001b[0m 0.9365  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.5425  \u001b[0m | \u001b[0m 0.5451  \u001b[0m | \u001b[0m 0.8782  \u001b[0m | \u001b[0m 6.633   \u001b[0m | \u001b[0m 5.028   \u001b[0m | \u001b[0m 0.1845  \u001b[0m | \u001b[0m 0.333   \u001b[0m | \u001b[0m 0.9125  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.9899  \u001b[0m | \u001b[0m 0.1132  \u001b[0m | \u001b[0m 0.4992  \u001b[0m | \u001b[0m 4.438   \u001b[0m | \u001b[0m 1.408   \u001b[0m | \u001b[0m 0.3822  \u001b[0m | \u001b[0m 0.59    \u001b[0m | \u001b[0m 0.3828  \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.01472 \u001b[0m | \u001b[95m 0.9447  \u001b[0m | \u001b[95m 0.1754  \u001b[0m | \u001b[95m 0.9888  \u001b[0m | \u001b[95m 3.962   \u001b[0m | \u001b[95m 1.968   \u001b[0m | \u001b[95m 0.143   \u001b[0m | \u001b[95m 0.9514  \u001b[0m | \u001b[95m 0.1721  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.3223  \u001b[0m | \u001b[0m 0.07107 \u001b[0m | \u001b[0m 0.9609  \u001b[0m | \u001b[0m 3.299   \u001b[0m | \u001b[0m 1.477   \u001b[0m | \u001b[0m 0.1185  \u001b[0m | \u001b[0m 0.9697  \u001b[0m | \u001b[0m 0.1459  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.9786  \u001b[0m | \u001b[0m 0.8253  \u001b[0m | \u001b[0m 0.426   \u001b[0m | \u001b[0m 3.594   \u001b[0m | \u001b[0m 1.125   \u001b[0m | \u001b[0m 0.00297 \u001b[0m | \u001b[0m 0.992   \u001b[0m | \u001b[0m 0.2421  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.3269  \u001b[0m | \u001b[0m 0.0387  \u001b[0m | \u001b[0m 0.7234  \u001b[0m | \u001b[0m 3.7     \u001b[0m | \u001b[0m 1.555   \u001b[0m | \u001b[0m 0.1763  \u001b[0m | \u001b[0m 0.07888 \u001b[0m | \u001b[0m 0.1042  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.1525  \u001b[0m | \u001b[0m 0.2097  \u001b[0m | \u001b[0m 0.8733  \u001b[0m | \u001b[0m 4.25    \u001b[0m | \u001b[0m 1.598   \u001b[0m | \u001b[0m 0.003849\u001b[0m | \u001b[0m 0.7693  \u001b[0m | \u001b[0m 0.2651  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.962   \u001b[0m | \u001b[0m 0.3398  \u001b[0m | \u001b[0m 0.09728 \u001b[0m | \u001b[0m 2.004   \u001b[0m | \u001b[0m 19.84   \u001b[0m | \u001b[0m 0.6885  \u001b[0m | \u001b[0m 0.7105  \u001b[0m | \u001b[0m 0.1482  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.01473 \u001b[0m | \u001b[0m 0.7023  \u001b[0m | \u001b[0m 0.5607  \u001b[0m | \u001b[0m 0.03768 \u001b[0m | \u001b[0m 2.08    \u001b[0m | \u001b[0m 11.52   \u001b[0m | \u001b[0m 0.215   \u001b[0m | \u001b[0m 0.8335  \u001b[0m | \u001b[0m 0.2034  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.1016  \u001b[0m | \u001b[0m 0.9685  \u001b[0m | \u001b[0m 0.9327  \u001b[0m | \u001b[0m 6.963   \u001b[0m | \u001b[0m 19.85   \u001b[0m | \u001b[0m 0.1497  \u001b[0m | \u001b[0m 0.9963  \u001b[0m | \u001b[0m 0.5257  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.01472 \u001b[0m | \u001b[0m 0.9811  \u001b[0m | \u001b[0m 0.07011 \u001b[0m | \u001b[0m 0.9138  \u001b[0m | \u001b[0m 6.909   \u001b[0m | \u001b[0m 1.665   \u001b[0m | \u001b[0m 0.2774  \u001b[0m | \u001b[0m 0.8756  \u001b[0m | \u001b[0m 0.2811  \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# train the optimizer on train set\n",
    "xbo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>subsample</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.897530</td>\n",
       "      <td>0.045712</td>\n",
       "      <td>0.662807</td>\n",
       "      <td>4.238468</td>\n",
       "      <td>1.435660</td>\n",
       "      <td>0.306424</td>\n",
       "      <td>0.713585</td>\n",
       "      <td>0.193055</td>\n",
       "      <td>-0.014719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.790404</td>\n",
       "      <td>0.644709</td>\n",
       "      <td>0.915190</td>\n",
       "      <td>3.334492</td>\n",
       "      <td>3.238280</td>\n",
       "      <td>0.777161</td>\n",
       "      <td>0.269010</td>\n",
       "      <td>0.972576</td>\n",
       "      <td>-0.014722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.849819</td>\n",
       "      <td>0.604370</td>\n",
       "      <td>0.687435</td>\n",
       "      <td>6.651023</td>\n",
       "      <td>15.698338</td>\n",
       "      <td>0.061001</td>\n",
       "      <td>0.511379</td>\n",
       "      <td>0.684811</td>\n",
       "      <td>-0.014721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.729727</td>\n",
       "      <td>0.851274</td>\n",
       "      <td>0.462704</td>\n",
       "      <td>4.756996</td>\n",
       "      <td>4.964748</td>\n",
       "      <td>0.932765</td>\n",
       "      <td>0.362983</td>\n",
       "      <td>0.936539</td>\n",
       "      <td>-0.014723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.542456</td>\n",
       "      <td>0.545092</td>\n",
       "      <td>0.878165</td>\n",
       "      <td>6.632704</td>\n",
       "      <td>5.028311</td>\n",
       "      <td>0.184497</td>\n",
       "      <td>0.333049</td>\n",
       "      <td>0.912511</td>\n",
       "      <td>-0.014722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.989919</td>\n",
       "      <td>0.113178</td>\n",
       "      <td>0.499188</td>\n",
       "      <td>4.438035</td>\n",
       "      <td>1.407600</td>\n",
       "      <td>0.382232</td>\n",
       "      <td>0.589994</td>\n",
       "      <td>0.382848</td>\n",
       "      <td>-0.014720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.944747</td>\n",
       "      <td>0.175445</td>\n",
       "      <td>0.988836</td>\n",
       "      <td>3.961777</td>\n",
       "      <td>1.968403</td>\n",
       "      <td>0.143046</td>\n",
       "      <td>0.951420</td>\n",
       "      <td>0.172068</td>\n",
       "      <td>-0.014719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.322294</td>\n",
       "      <td>0.071075</td>\n",
       "      <td>0.960918</td>\n",
       "      <td>3.298961</td>\n",
       "      <td>1.476775</td>\n",
       "      <td>0.118518</td>\n",
       "      <td>0.969743</td>\n",
       "      <td>0.145927</td>\n",
       "      <td>-0.014719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.978550</td>\n",
       "      <td>0.825309</td>\n",
       "      <td>0.426020</td>\n",
       "      <td>3.593515</td>\n",
       "      <td>1.124848</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>0.992008</td>\n",
       "      <td>0.242077</td>\n",
       "      <td>-0.014720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.326916</td>\n",
       "      <td>0.038696</td>\n",
       "      <td>0.723418</td>\n",
       "      <td>3.699749</td>\n",
       "      <td>1.555163</td>\n",
       "      <td>0.176325</td>\n",
       "      <td>0.078883</td>\n",
       "      <td>0.104183</td>\n",
       "      <td>-0.014720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.152486</td>\n",
       "      <td>0.209724</td>\n",
       "      <td>0.873266</td>\n",
       "      <td>4.250220</td>\n",
       "      <td>1.598034</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.769316</td>\n",
       "      <td>0.265085</td>\n",
       "      <td>-0.014719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.961977</td>\n",
       "      <td>0.339810</td>\n",
       "      <td>0.097284</td>\n",
       "      <td>2.004220</td>\n",
       "      <td>19.843033</td>\n",
       "      <td>0.688511</td>\n",
       "      <td>0.710459</td>\n",
       "      <td>0.148205</td>\n",
       "      <td>-0.014724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.702325</td>\n",
       "      <td>0.560722</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>2.080232</td>\n",
       "      <td>11.522424</td>\n",
       "      <td>0.214985</td>\n",
       "      <td>0.833452</td>\n",
       "      <td>0.203444</td>\n",
       "      <td>-0.014728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.968504</td>\n",
       "      <td>0.932706</td>\n",
       "      <td>6.963342</td>\n",
       "      <td>19.853885</td>\n",
       "      <td>0.149651</td>\n",
       "      <td>0.996273</td>\n",
       "      <td>0.525734</td>\n",
       "      <td>-0.014720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.981059</td>\n",
       "      <td>0.070108</td>\n",
       "      <td>0.913761</td>\n",
       "      <td>6.908951</td>\n",
       "      <td>1.664606</td>\n",
       "      <td>0.277445</td>\n",
       "      <td>0.875619</td>\n",
       "      <td>0.281147</td>\n",
       "      <td>-0.014719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    colsample_bytree     gamma  learning_rate  max_depth  min_child_weight  \\\n",
       "0           0.897530  0.045712       0.662807   4.238468          1.435660   \n",
       "1           0.790404  0.644709       0.915190   3.334492          3.238280   \n",
       "2           0.849819  0.604370       0.687435   6.651023         15.698338   \n",
       "3           0.729727  0.851274       0.462704   4.756996          4.964748   \n",
       "4           0.542456  0.545092       0.878165   6.632704          5.028311   \n",
       "5           0.989919  0.113178       0.499188   4.438035          1.407600   \n",
       "6           0.944747  0.175445       0.988836   3.961777          1.968403   \n",
       "7           0.322294  0.071075       0.960918   3.298961          1.476775   \n",
       "8           0.978550  0.825309       0.426020   3.593515          1.124848   \n",
       "9           0.326916  0.038696       0.723418   3.699749          1.555163   \n",
       "10          0.152486  0.209724       0.873266   4.250220          1.598034   \n",
       "11          0.961977  0.339810       0.097284   2.004220         19.843033   \n",
       "12          0.702325  0.560722       0.037680   2.080232         11.522424   \n",
       "13          0.101560  0.968504       0.932706   6.963342         19.853885   \n",
       "14          0.981059  0.070108       0.913761   6.908951          1.664606   \n",
       "\n",
       "    reg_alpha  reg_lambda  subsample      rmse  \n",
       "0    0.306424    0.713585   0.193055 -0.014719  \n",
       "1    0.777161    0.269010   0.972576 -0.014722  \n",
       "2    0.061001    0.511379   0.684811 -0.014721  \n",
       "3    0.932765    0.362983   0.936539 -0.014723  \n",
       "4    0.184497    0.333049   0.912511 -0.014722  \n",
       "5    0.382232    0.589994   0.382848 -0.014720  \n",
       "6    0.143046    0.951420   0.172068 -0.014719  \n",
       "7    0.118518    0.969743   0.145927 -0.014719  \n",
       "8    0.002970    0.992008   0.242077 -0.014720  \n",
       "9    0.176325    0.078883   0.104183 -0.014720  \n",
       "10   0.003849    0.769316   0.265085 -0.014719  \n",
       "11   0.688511    0.710459   0.148205 -0.014724  \n",
       "12   0.214985    0.833452   0.203444 -0.014728  \n",
       "13   0.149651    0.996273   0.525734 -0.014720  \n",
       "14   0.277445    0.875619   0.281147 -0.014719  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimization results (or xbo.optimization_results_)\n",
    "xbo.get_optimization_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>subsample</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.944747</td>\n",
       "      <td>0.175445</td>\n",
       "      <td>0.988836</td>\n",
       "      <td>3.961777</td>\n",
       "      <td>1.968403</td>\n",
       "      <td>0.143046</td>\n",
       "      <td>0.951420</td>\n",
       "      <td>0.172068</td>\n",
       "      <td>-0.014719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.322294</td>\n",
       "      <td>0.071075</td>\n",
       "      <td>0.960918</td>\n",
       "      <td>3.298961</td>\n",
       "      <td>1.476775</td>\n",
       "      <td>0.118518</td>\n",
       "      <td>0.969743</td>\n",
       "      <td>0.145927</td>\n",
       "      <td>-0.014719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   colsample_bytree     gamma  learning_rate  max_depth  min_child_weight  \\\n",
       "0          0.944747  0.175445       0.988836   3.961777          1.968403   \n",
       "1          0.322294  0.071075       0.960918   3.298961          1.476775   \n",
       "\n",
       "   reg_alpha  reg_lambda  subsample      rmse  \n",
       "0   0.143046    0.951420   0.172068 -0.014719  \n",
       "1   0.118518    0.969743   0.145927 -0.014719  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best performance (or xbo.best_performance_)\n",
    "xbo.get_best_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9447466905645845,\n",
       " 'gamma': 0.17544515751269651,\n",
       " 'learning_rate': 0.9888359157005454,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1.9684029067570108,\n",
       " 'reg_alpha': 0.14304561710132546,\n",
       " 'reg_lambda': 0.9514199873030839,\n",
       " 'subsample': 0.17206756072404922}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuned params (or xbo.best_params_)\n",
    "xbo.get_best_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bayes_opt.bayesian_optimization.BayesianOptimization at 0x7fe06f4959d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer object (or xbo.optimizer_)\n",
    "xbo.get_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': (2, 7),\n",
       " 'learning_rate': (0, 1),\n",
       " 'min_child_weight': (1, 20),\n",
       " 'colsample_bytree': (0.1, 1.0),\n",
       " 'subsample': (0.1, 1),\n",
       " 'gamma': (0, 1),\n",
       " 'reg_alpha': (0, 1),\n",
       " 'reg_lambda': (0, 1)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimization params boundaries or (xbo.pbounds)\n",
    "xbo.get_pbounds()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
